{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### DEPENDENCIES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from collections import Counter\n",
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# data analysis\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from numpy import isneginf\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from sklearn.manifold import TSNE\n",
    "import scipy.stats\n",
    "\n",
    "# plots\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.lines import Line2D\n",
    "import seaborn as sns\n",
    "\n",
    "# notebook settings\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "np.seterr(divide = 'ignore')                 # divide by zero warning log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PREPARATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data():\n",
    "    \"\"\"\n",
    "    Read data from music task from folder /data.\n",
    "    @return\n",
    "        data: pandas dataframe with all data from pairs concatenated\n",
    "    \"\"\"\n",
    "        \n",
    "    # Search file names\n",
    "    filenames = []\n",
    "    for root, dirs, files in os.walk(\"data\", topdown=False):\n",
    "        for name in files:\n",
    "            if name.startswith(\"turns\") and name.endswith(\".txt\"):\n",
    "                filenames.append(os.path.join(root, name))\n",
    "    \n",
    "    filenames = sorted(filenames)            \n",
    "\n",
    "    # Construct data frame\n",
    "    sep = '|'\n",
    "    frames = []\n",
    "    \n",
    "    for file in filenames:\n",
    "        data = pd.read_csv(\n",
    "                    file, \n",
    "                    sep = '|',\n",
    "                    na_values={'note_p1': [' '], \"note_p2\": [' ']},\n",
    "                    names=[\"experiment_id\",  # The timestamp when the experiment started\n",
    "                           \"game_number\",    # Game number. Incremented by 1 each time they succeed OR timeout\n",
    "                           \"level\",          # IGNORE THIS COLUMN\n",
    "                           \"score\",          # IGNORE THIS COLUMN\n",
    "                           \"level_2\",        # IGNORE THIS COLUMN\n",
    "                           \"participant\",    # The actual, physical participant. P1 OR P2\n",
    "                           \"role\",           # The role of the participant. director OR matcher OR NEWGAME\n",
    "                           \"timestamp\",      # The timestamp when the data on that row was saved. Unix time\n",
    "                           \"note_p1\",        # NEWGAME, NaN, C3ON, C3OFF, E3ON, E3OFF, G3ON, G3OFF\n",
    "                           \"note_p2\",        # A3ON, A3OFF, D3ON, D3OFF, F3ON, F3OFF\n",
    "                           \"pressure\",       # The \"pressure\" of the keypress (between 1 and 100)\n",
    "                           \"note_check\",     # Records whether the particular note press/release was the correct note or NOT. NEWGAME, CORRECT, COMPLETE, ERROR, TIMEOUT\n",
    "                           \"succes_status\",  # The first number is how many chords have been successfully completed. The second number is the length of the sequence (i.e. how many \"chords\")\n",
    "                           \"report_status\",  # This column stores for each note of each chord when the key was pressed and when the key was released\n",
    "                           \"difficulty\"],    # These are just the difficulty settings for the random chord generator. You can ignore this.  \n",
    "                     dtype={\"note_p1\":pd.StringDtype(),\n",
    "                            \"note_p2\":pd.StringDtype()}) \n",
    "        \n",
    "        data.insert(0, \"pair\", file[5:11], True) # pair number\n",
    "        frames.append(data)\n",
    "    \n",
    "    data = pd.concat(frames)\n",
    "    data = data.reset_index(drop=True)\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_succesful_sequences(data):\n",
    "    \"\"\"\n",
    "    Filter out the rows that are part of the successful sequences that determine the end of the game round.\n",
    "    @params\n",
    "        data: pandas dataframe with data from the music task\n",
    "    @return\n",
    "        data: copy? of original dataframe without the succesful rows\n",
    "    \"\"\"\n",
    "    \n",
    "    f = [True for i in data.index] # filter: True = keep, False = remove\n",
    "    \n",
    "    for i in data.index:\n",
    "        \n",
    "        if i > 0: #ignore first 'NEW GAME'\n",
    "            if data.loc[i, 'note_check'] == 'NEWGAME':\n",
    "            \n",
    "                # check for succesful sequence\n",
    "                j = i-1\n",
    "                previous_row = data.iloc[j]\n",
    "                \n",
    "                while previous_row['note_check'] == 'COMPLETE' or previous_row['note_check'] == 'CORRECT':\n",
    "                    f[j] = False\n",
    "                    j -= 1\n",
    "                    previous_row = data.iloc[j]\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            pass\n",
    "\n",
    "    # apply filter\n",
    "    data = data[f]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_key_setup(data):\n",
    "    \"\"\"\n",
    "    Add a column with info about keyboard setup: 'parallel' or 'overlap'.\n",
    "    @params\n",
    "        data: pandas dataframe with data from the music task\n",
    "    @return\n",
    "        data: original dataframe plus an extra column with key setup\n",
    "    \"\"\"\n",
    "    \n",
    "    # parallel/overlap pairs\n",
    "    parallel = ['pair02', 'pair03', 'pair06', 'pair07', 'pair09', 'pair11', 'pair13', 'pair15', 'pair16', 'pair18', 'pair19', 'pair20', 'pair23', 'pair24', 'pair27']\n",
    "    overlap = ['pair04', 'pair05', 'pair08', 'pair10', 'pair12', 'pair14', 'pair17', 'pair21', 'pair22', 'pair25', 'pair26']\n",
    "\n",
    "    # add setup\n",
    "    data['setup'] = 'parallel'\n",
    "    data.loc[data['pair'].isin(overlap), 'setup'] = 'overlap'\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_key_numbers(data):\n",
    "    \"\"\"\n",
    "    Translate note letters to key numbers depending on participant, role and key setup.\n",
    "    In pair 13 and 14, some key presses were logged wrong. In that case, the note letters are translated as 'ERR'.\n",
    "    @params\n",
    "        data: pandas dataframe with data from the music task\n",
    "    @return\n",
    "        data: orginal dataframe plus an extra column for each participant containing the translated keys\n",
    "    \"\"\"\n",
    "    \n",
    "    # translation for parallel key setup\n",
    "    t_dict_p = {\n",
    "        'note_p1': { \n",
    "            'C3ON': '1ON', 'C3OFF': '1OFF',\n",
    "            'E3ON': '2ON', 'E3OFF': '2OFF',\n",
    "            'G3ON': '3ON', 'G3OFF': '3OFF'\n",
    "        },\n",
    "        'note_p2': {\n",
    "            'D3ON': '4ON', 'D3OFF': '4OFF',\n",
    "            'F3ON': '5ON', 'F3OFF': '5OFF',\n",
    "            'A3ON': '6ON', 'A3OFF': '6OFF'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    # translation for overlapping key setup\n",
    "    t_dict_o = {\n",
    "        'note_p1': { \n",
    "            'C3ON': '1ON', 'C3OFF': '1OFF',\n",
    "            'D3ON': '2ON', 'D3OFF': '2OFF',\n",
    "            'E3ON': '3ON', 'E3OFF': '3OFF'\n",
    "        },\n",
    "        'note_p2': {\n",
    "            'E3ON': '4ON', 'E3OFF': '4OFF',\n",
    "            'G3ON': '5ON', 'G3OFF': '5OFF',\n",
    "            'F3ON': '6ON', 'F3OFF': '6OFF'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    key_p1 = []\n",
    "    key_p2 = []\n",
    "\n",
    "    for i in data.index:\n",
    "\n",
    "        if data.loc[i, 'note_check'] == 'NEWGAME':\n",
    "            key_p1.append('NEWGAME')\n",
    "            key_p2.append('NEWGAME')            \n",
    "        \n",
    "        elif data.loc[i, 'note_check'] == 'TIMEOUT':\n",
    "            key_p1.append('TIMEOUT')\n",
    "            key_p2.append('TIMEOUT')\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            # Key setup = parallel\n",
    "            if data.loc[i, 'setup'] == 'parallel':\n",
    "        \n",
    "                if data.loc[i, 'participant'] == 'P1':\n",
    "                    \n",
    "                    try:\n",
    "                        keypress = data.loc[i, 'note_p1'].strip()\n",
    "                        translation = t_dict_p['note_p1'][keypress]\n",
    "                        key_p1.append(translation)\n",
    "                        key_p2.append(np.nan)\n",
    "                    \n",
    "                    except:\n",
    "                        key_p1.append(\"ERR\")\n",
    "                        key_p2.append(np.nan)\n",
    "                        print(\"ERROR: Prohibited key at %s, %s.\" % (i, data.loc[i, 'pair']))\n",
    "                    \n",
    "                elif data.loc[i, 'participant'] == 'P2':\n",
    "                    \n",
    "                    try:\n",
    "                        keypress = data.loc[i, 'note_p2'].strip()\n",
    "                        translation = t_dict_p['note_p2'][keypress]\n",
    "                        key_p2.append(translation)\n",
    "                        key_p1.append(np.nan)\n",
    "                    \n",
    "                    except:\n",
    "                        key_p2.append(\"ERR\")\n",
    "                        key_p1.append(np.nan)\n",
    "                        print(\"ERROR: Prohibited key at %s, %s.\" % (i, data.loc[i, 'pair']))\n",
    "                    \n",
    "                    \n",
    "                else:\n",
    "                    print(\"issue here\")\n",
    "                    pass\n",
    "        \n",
    "            # Key setup = overlap\n",
    "            else:\n",
    "        \n",
    "                if data.loc[i, 'participant']  == 'P1':\n",
    "                    \n",
    "                    try:\n",
    "                        keypress = data.loc[i, 'note_p1'].strip()\n",
    "                        translation = t_dict_o['note_p1'][keypress]\n",
    "                        key_p1.append(translation)\n",
    "                        key_p2.append(np.nan)\n",
    "                    \n",
    "                    except:\n",
    "                        key_p1.append(\"ERR\")\n",
    "                        key_p2.append(np.nan)\n",
    "                        print(\"ERROR: Prohibited key at %s, %s.\" % (i, data.loc[i, 'pair']))\n",
    "                    \n",
    "                elif data.loc[i, 'participant'] == 'P2':\n",
    "                    \n",
    "                    try:\n",
    "                        keypress = data.loc[i, 'note_p2'].strip()\n",
    "                        translation = t_dict_o['note_p2'][keypress]\n",
    "                        key_p2.append(translation)\n",
    "                        key_p1.append(np.nan)\n",
    "                    \n",
    "                    except:\n",
    "                        key_p2.append(\"ERR\")\n",
    "                        key_p1.append(np.nan)\n",
    "                        print(\"ERROR: Prohibited key at %s, %s.\" % (i, data.loc[i, 'pair']))\n",
    "                    \n",
    "                else:\n",
    "                    print(\"issue here\")\n",
    "                    pass\n",
    "\n",
    "    # Append columns\n",
    "    data['key_p1'] = key_p1\n",
    "    data['key_p2'] = key_p2\n",
    "                \n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess(data):\n",
    "    \"\"\"\n",
    "    Preprocessing consists of five parts:\n",
    "        1. Remove rows that are part of successful sequences, before a game is completed\n",
    "        2. Add a column with the key setup\n",
    "        3. Add columns for keynumbers of P1 and P2\n",
    "        4. Drop unused columns\n",
    "        5. Reorder columns\n",
    "    @params\n",
    "        data: pandas dataframe with data from the music task\n",
    "    @return\n",
    "        data: modified selection of the original dataframe\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Remove rows that are part of succesful sequences\n",
    "    data = preprocess_succesful_sequences(data)\n",
    "    \n",
    "    # 2. Add column with key setup\n",
    "    data = preprocess_key_setup(data)\n",
    "    \n",
    "    # 3. Add columns with key numbers  \n",
    "    data = preprocess_key_numbers(data)\n",
    "    \n",
    "    # 4. Drop unused columns\n",
    "    unused = [\n",
    "        'experiment_id',\n",
    "        'level',\n",
    "        'level_2',\n",
    "        'game_number',\n",
    "        'timestamp',\n",
    "        'pressure',\n",
    "        'note_check',\n",
    "        'succes_status',\n",
    "        'report_status',\n",
    "        'difficulty',\n",
    "        'note_p1',\n",
    "        'note_p2',\n",
    "    ]\n",
    "    data = data.drop(columns = unused)\n",
    "    \n",
    "    # 5. Reorder columns\n",
    "    order = [\n",
    "        'pair',\n",
    "        'setup',\n",
    "        'score',\n",
    "        'participant',\n",
    "        'role',\n",
    "        'key_p1',\n",
    "        'key_p2'\n",
    "    ]\n",
    "    \n",
    "    data = data[order]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### A. PARSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def structure_stream(data):\n",
    "    \"\"\" \n",
    "    Structure the keypresses based on game rounds and turns between participants.\n",
    "    The distinguishable splits in the stream are represented in a nested list and tuple structure.\n",
    "    ERR keys are ignored.\n",
    "    @params \n",
    "        data: pandas dataframe of a pair (relevant columns are: participant, note_p1, note_p2, note_check)\n",
    "    @return\n",
    "        experiment: nested list and tuple [game: [turn: (participant, role, [keypresses])]]\n",
    "    @TODO\n",
    "        replace iterrows with data.index\n",
    "    \"\"\"\n",
    "    \n",
    "    # experiment = [game, game]\n",
    "    experiment = []\n",
    "    \n",
    "    # game = [turn, turn]\n",
    "    game = []\n",
    "    \n",
    "    # turn = (participant, role, [keypresses])\n",
    "    participant = ''\n",
    "    role = ''\n",
    "    \n",
    "    # keypresses = ['AON', 'AOFF', etc.]\n",
    "    keypresses = []\n",
    "    \n",
    "    for i, row in data.iterrows():\n",
    "    \n",
    "        # First row: 'NEWGAME'\n",
    "        if i == 0:\n",
    "            # Ignore\n",
    "            pass\n",
    "        \n",
    "        # Last row: end of experiment\n",
    "        elif i == len(data.index)-1:\n",
    "            \n",
    "            if row.key_p1 == 'NEWGAME' or row.key_p1 == 'TIMEOUT':\n",
    "                # Ignore\n",
    "                pass\n",
    "\n",
    "            elif row.key_p1 == 'ERR' or row.key_p2 == 'ERR':\n",
    "                # Ignore\n",
    "                pass\n",
    "            \n",
    "            else:\n",
    "                if row.participant == 'P1':\n",
    "                    keypresses.append(row.key_p1)\n",
    "            \n",
    "                elif row.participant == 'P2':\n",
    "                    keypresses.append(row.key_p2)\n",
    "                \n",
    "                else:\n",
    "                    pass\n",
    "            \n",
    "            # End of experiment: add turn to game\n",
    "            if len(keypresses) > 0 : game.append((participant, role, keypresses))\n",
    "            \n",
    "            # End of experiment: add game to stream\n",
    "            if len(game) > 0 : experiment.append(game)\n",
    "        \n",
    "        # Within experiment\n",
    "        else:\n",
    "            \n",
    "            next_row = data.iloc[i+1]\n",
    "            \n",
    "            # End of game\n",
    "            if row.key_p1 == 'NEWGAME' or row.key_p1 == 'TIMEOUT':\n",
    "\n",
    "                # End of game: add turn to game\n",
    "                if len(keypresses) > 0 : game.append((participant, role, keypresses))\n",
    "                # Reset\n",
    "                keypresses = []\n",
    "                participant = ''\n",
    "                role = '' \n",
    "\n",
    "                # End of game: add game to experiment\n",
    "                if len(game) > 0 : experiment.append(game)\n",
    "                # Reset game\n",
    "                game = []\n",
    "            \n",
    "            elif row.key_p1 == 'ERR' or row.key_p2 == 'ERR':\n",
    "                # Ignore\n",
    "                pass\n",
    "            \n",
    "            # Within game\n",
    "            else:\n",
    "\n",
    "                if row.participant == 'P1':\n",
    "\n",
    "                    participant = row.participant\n",
    "                    role = row.role\n",
    "                    keypresses.append(row.key_p1)\n",
    "\n",
    "                    if next_row.participant == 'P2':\n",
    "                        # End of turn: append turn to game\n",
    "                        game.append((participant, role, keypresses))\n",
    "                        # Reset\n",
    "                        keypresses = []\n",
    "                        participant = ''\n",
    "                        role = '' \n",
    "\n",
    "                elif row.participant == 'P2':\n",
    "\n",
    "                    participant = row.participant\n",
    "                    role = row.role\n",
    "                    keypresses.append(row.key_p2)\n",
    "\n",
    "                    if next_row.participant == 'P1':\n",
    "                        # End of turn: append turn to game\n",
    "                        game.append((participant, role, keypresses))\n",
    "                        # Reset\n",
    "                        keypresses = []\n",
    "                        participant = ''\n",
    "                        role = ''\n",
    "\n",
    "                else:\n",
    "                    pass\n",
    "    \n",
    "    return experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(data):\n",
    "    \"\"\"\n",
    "    Use structured stream to first distinguish splits based on game and turn.\n",
    "    Parses on chord level: keypresses are stacked until all keys are released, then the stack is simplified \n",
    "    to only the distinct notes without considering the order of occurrence.\n",
    "    This means that overlapping terms, e.g. when participants interrupt one another, are taken as one term.\n",
    "    Example: DON, AON, DOFF, CON, COFF, AOFF > ADC\n",
    "    \n",
    "    @params\n",
    "        data: pandas dataframe (relevant columns are: participant, note_p1, note_p2, note_check)\n",
    "        mark_role: annotates terms by role of the participant that uses the term\n",
    "        add_split: mark splits of games with NEW_GAME\n",
    "    @return\n",
    "        stream: list of lists of tokens\n",
    "    \"\"\"\n",
    "    \n",
    "    # [experiment [game (participant, role, ['AON', 'AOFF'])]]\n",
    "    structured_stream = structure_stream(data)\n",
    "    \n",
    "    # for dataframe(columns: term, participants, roles)\n",
    "    stream = []\n",
    "    \n",
    "    # game\n",
    "    for game in structured_stream:\n",
    "        \n",
    "        # take one game to be a \"sentence\"\n",
    "        sentence = []\n",
    "        \n",
    "        stack = []\n",
    "        P1 = False\n",
    "        P2 = False\n",
    "        director = False\n",
    "        matcher = False\n",
    "        \n",
    "        # Turn\n",
    "        for participant, role, keypresses in game:\n",
    "            \n",
    "            keys = [key[0] for key in keypresses]\n",
    "            \n",
    "            for key in keys:\n",
    "            \n",
    "                if participant == 'P1': P1 = True\n",
    "                if participant == 'P2': P2 = True\n",
    "                if role == 'director': director = True\n",
    "                if role == 'matcher': matcher = True\n",
    "            \n",
    "                stack.append(key)\n",
    "                \n",
    "                # All keys are released for both participants: a term is formed\n",
    "                if all(value%2 == 0 for value in Counter(k for k in stack).values()):\n",
    "                    \n",
    "                    # Term column\n",
    "                    term = ''.join(sorted(list(set(stack))))\n",
    "                    \n",
    "                    if director and matcher:\n",
    "                        term = term # don't have to annotate if mixed, right?\n",
    "                    elif director:\n",
    "                        term = 'd_' + term\n",
    "                    else: # matcher\n",
    "                        term = 'm_' + term\n",
    "                    \n",
    "                    sentence.append(term)\n",
    "                    \n",
    "                    # Reset\n",
    "                    stack = []\n",
    "                    P1 = False\n",
    "                    P2 = False\n",
    "                    director = False\n",
    "                    matcher = False\n",
    "        \n",
    "        # Append sentence to stream\n",
    "        stream.append(sentence)\n",
    "    \n",
    "    return stream"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B. CO-OCCURRENCES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_voc(voc):\n",
    "    \"\"\"\n",
    "    Sort vocabulary alphabetically and then based on length of term.\n",
    "    The order of the vocabulary is relevant once the corresponding matrices are visualized or further computed.\n",
    "    \n",
    "    @ params\n",
    "        voc: list of individual terms\n",
    "        mark_role: whether the prefix d_ or m_ is part of the terms\n",
    "    @ return\n",
    "        ordered_voc: list of term ordered in such a way that the heatmap of the matrix will be better readable\n",
    "    \"\"\"\n",
    "    \n",
    "    # Alphabetically\n",
    "    sorted_voc = sorted(voc)\n",
    "    \n",
    "    # Length term\n",
    "    sorted_voc.sort(key=lambda x: len(x))\n",
    "    \n",
    "    return sorted_voc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def matrix(stream, n=3):\n",
    "    \"\"\"\n",
    "    Compute co-occurrence numpy matrix based on n context window at both sides.\n",
    "    \n",
    "    @ params\n",
    "        df_stream: pandas dataframe with relevant column (term)\n",
    "        n: size of context window\n",
    "    @ return\n",
    "        matrix: numpy matrix size len(voc) x len(voc) with counts how many times term with term in vicinity of n terms occurs\n",
    "        voc_dict: with as key all terms in the vocabulary and as value their index corresponding to the matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # Vocabulary\n",
    "    voc = sort_voc(list(set([term for sentence in stream for term in sentence])))\n",
    "    voc_dict = {voc[i]:i for i in range(len(voc))}\n",
    "    \n",
    "    # Numpy matrix [len(voc) x len(voc)]\n",
    "    matrix = []\n",
    "    \n",
    "    for word in voc:\n",
    "        \n",
    "        # Each row corresponds to a word\n",
    "        row = [0 for i in range(len(voc))]\n",
    "        \n",
    "        for sentence in stream: \n",
    "        \n",
    "            for i in range(len(sentence)):\n",
    "            \n",
    "                if sentence[i] == word:    \n",
    "\n",
    "                    left = sentence[i-n:i] if i-n > 0 else []\n",
    "                    right = sentence[i+1:i+1+n] if i+1+n <= len(sentence) else []\n",
    "                    context = left + right\n",
    "\n",
    "                    for context_word in context:\n",
    "                        row[voc_dict[context_word]] += 1\n",
    "        \n",
    "        matrix.append(row)\n",
    "    \n",
    "    matrix = np.array(matrix)\n",
    "    \n",
    "    return matrix, voc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_embeddings(title, matrix, voc_dict, save_fig=False, show_fig=False):\n",
    "    \"\"\"\n",
    "    Visualize reduced matrix with a scatterplot. \n",
    "    @ params\n",
    "        title: title for plot\n",
    "        matrix: numpy co-occurrence matrix\n",
    "        voc_dict: all terms corresponding to indexes of matrix {term: index} to annotate plot \n",
    "    @ output\n",
    "        embeddings: 2d graph scatterplot of reduced vector space\n",
    "    \"\"\"\n",
    "    \n",
    "    if save_fig or show_fig:\n",
    "        # Get normalized matrix: makes relationships more pronounced\n",
    "        # matrix = normalize(matrix)\n",
    "    \n",
    "        # Get reduced matrix\n",
    "        tsne = TSNE(n_components=2, perplexity=30, early_exaggeration=12.0, random_state=0)\n",
    "        reduced_matrix = tsne.fit_transform(matrix) \n",
    "    \n",
    "        # Get x- and y-choords\n",
    "        x = [term[0] for term in reduced_matrix]\n",
    "        y = [term[1] for term in reduced_matrix]\n",
    "        labels = voc_dict.keys()\n",
    "    \n",
    "        # Plot\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        ax = sns.scatterplot(x, y)\n",
    "    \n",
    "        # Different colors for P1_d, P1_m, P2_d, P2_m\n",
    "        terms_p1_d = [term for term in voc_dict.keys() if term.startswith('d_') and term[2] in ['1', '2', '3']]\n",
    "        terms_p1_m = [term for term in voc_dict.keys() if term.startswith('m_') and term[2] in ['1', '2', '3']]\n",
    "        terms_p2_d = [term for term in voc_dict.keys() if term.startswith('d_') and term[2] in ['4', '5', '6']]\n",
    "        terms_p2_m = [term for term in voc_dict.keys() if term.startswith('m_') and term[2] in ['4', '5', '6']]\n",
    "    \n",
    "        for term in terms_p1_d:\n",
    "            ax.plot(x[voc_dict[term]], y[voc_dict[term]], color = 'C1', marker=\"o\")\n",
    "        for term in terms_p1_m:\n",
    "            ax.plot(x[voc_dict[term]], y[voc_dict[term]], color = 'C1', marker=\"o\")\n",
    "        for term in terms_p2_d:\n",
    "            ax.plot(x[voc_dict[term]], y[voc_dict[term]], color = 'C2', marker=\"o\")\n",
    "        for term in terms_p2_m:\n",
    "            ax.plot(x[voc_dict[term]], y[voc_dict[term]], color = 'C2', marker=\"o\")\n",
    "    \n",
    "        # Label dots with terms\n",
    "        for i, txt in enumerate(labels):\n",
    "            ax.annotate(txt, (x[i], y[i]))\n",
    "    \n",
    "        # Legend\n",
    "        legend_elements = [Line2D([0], [0], marker='o', color='w', label='P1', markerfacecolor='C1', markersize=8),\n",
    "                          Line2D([0], [0], marker='o', color='w', label='P2', markerfacecolor='C2', markersize=8),\n",
    "                          Line2D([0], [0], marker='o', color='w', label='both', markerfacecolor='C0')]\n",
    "    \n",
    "        ax.legend(handles=legend_elements)\n",
    "        \n",
    "        plt.title(\"word vectors (t-SNE)\\n%s\" % title)\n",
    "        \n",
    "    if save_fig:\n",
    "        plt.savefig(\"graphs/word_vectors_tsne_%s.png\" % title, bbox_inches='tight')\n",
    "        \n",
    "    if show_fig: \n",
    "        plt.show()\n",
    "    \n",
    "    if save_fig and not show_fig:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C. STATEMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def table_distances(title, matrix, voc_dict, save_fig=False, show_fig=False):\n",
    "    \"\"\"\n",
    "    Calculate cosine distances between all terms based on co-occurrence matrix.\n",
    "    Reorganize table with director keys on rows and matcher keys on columns.\n",
    "    params:\n",
    "        matrix: numpy co-occurrence matrix\n",
    "        voc_dict: all terms corresponding to indexes of matrix {term: index} to annotate plot\n",
    "    return:\n",
    "        distances: table with cosine distances of selection\n",
    "    todo: \n",
    "        add mask to clean 0.99?\n",
    "    \"\"\"\n",
    "    \n",
    "    # Cosine distance function from sklearn\n",
    "    distances = cosine_distances(matrix)\n",
    "    \n",
    "    # Select terms for table    \n",
    "    # Director\n",
    "    i = [term for term in voc_dict.keys() if term.startswith('d_') and len(term) < 5]\n",
    "    # Matcher\n",
    "    j = [term for term in voc_dict.keys() if term.startswith('m_') and len(term) < 5]\n",
    "    \n",
    "    # Find indices of terms\n",
    "    indices_i = [voc_dict[term] for term in i]\n",
    "    indices_j = [voc_dict[term] for term in j]\n",
    "    \n",
    "    # Selection with indices \n",
    "    table = distances[np.ix_(indices_i, indices_j)]\n",
    "    \n",
    "    if save_fig or show_fig:\n",
    "        # Show results in heatmap\n",
    "        plt.figure(figsize=(len(i)*0.7, len(j)*0.7))\n",
    "        ax = sns.heatmap(table, xticklabels=j, yticklabels=i, annot=True, cbar=False, linewidths=0.05, linecolor='white', cmap=\"Blues_r\", vmin=0, vmax=1)\n",
    "        ax.xaxis.tick_top()\n",
    "        ax.title.set_position([.5, 1.05])\n",
    "        plt.yticks(rotation=0)\n",
    "        plt.title(\"cosine distance\\n%s\" % title)\n",
    "        \n",
    "        if save_fig:\n",
    "            plt.savefig(\"graphs/cosine_distance_%s.png\" % title,  bbox_inches='tight')\n",
    "        \n",
    "        if show_fig:\n",
    "            plt.show()\n",
    "        \n",
    "        if save_fig and not show_fig:\n",
    "            plt.close()\n",
    "\n",
    "    return distances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verbatim_translation_measure(table, voc_dict):\n",
    "    \"\"\" Calculate m_vt.\"\"\"\n",
    "    \n",
    "    # Average d_3 <to> m_4, d_4 <to> m_3\n",
    "    vt = [ table[voc_dict[\"d_3\"], voc_dict[\"m_4\"]],\n",
    "           table[voc_dict[\"d_4\"], voc_dict[\"m_3\"]] ]\n",
    "    \n",
    "    m_vt = sum(vt) / len(vt)\n",
    "    \n",
    "    return m_vt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def proximity_translation_measure(table, voc_dict):\n",
    "    \"\"\" Calculate m_pt.\"\"\"\n",
    "    \n",
    "    # Average d_12 <to> m_4, d_23 <to> m_5, d_45 <to> m_2, d_56 <to> m_3\n",
    "    pt = [ table[voc_dict[\"d_12\"], voc_dict[\"m_4\"]],\n",
    "               table[voc_dict[\"d_23\"], voc_dict[\"m_5\"]],\n",
    "               table[voc_dict[\"d_45\"], voc_dict[\"m_2\"]],\n",
    "               table[voc_dict[\"d_56\"], voc_dict[\"m_3\"]] ]\n",
    "\n",
    "    m_pt = sum(pt) / len(pt)\n",
    "    \n",
    "    return m_pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keys_mapping_measure(table, voc_dict):\n",
    "    \"\"\"Calculate m_km.\"\"\"\n",
    "    \n",
    "    # Single keys\n",
    "    i = [term for term in voc_dict.keys() if term.startswith('d_') and len(term) == 3]\n",
    "    j = [term for term in voc_dict.keys() if term.startswith('m_') and len(term) == 3]\n",
    "    indices_i = [voc_dict[term] for term in i]\n",
    "    indices_j = [voc_dict[term] for term in j]\n",
    "    t = table[np.ix_(indices_i, indices_j)]\n",
    "    \n",
    "    # Calculate average lowest over rows\n",
    "    km = [min(row) for row in t]\n",
    "    m_km = sum(km) / len(km)\n",
    "    \n",
    "    return m_km"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chords_mapping_measure(table, voc_dict):\n",
    "    \"\"\"Calculate m_cm.\"\"\"\n",
    "    \n",
    "    # Chords\n",
    "    i = [term for term in voc_dict.keys() if term.startswith('d_') and len(term) <= 4]\n",
    "    j = [term for term in voc_dict.keys() if term.startswith('m_') and len(term) <= 4]\n",
    "    indices_i = [voc_dict[term] for term in i]\n",
    "    indices_j = [voc_dict[term] for term in j]\n",
    "    t = table[np.ix_(indices_i, indices_j)]\n",
    "    \n",
    "    # Calculate average lowest over rows\n",
    "    cm = [min(row) for row in t]\n",
    "    m_cm = sum(cm) / len(cm)\n",
    "    \n",
    "    return m_cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_measures(table, voc_dict, print_results=False):\n",
    "    \"\"\"\n",
    "    Calculate all measures.\n",
    "    \"\"\"\n",
    "\n",
    "    # Measures\n",
    "    m_vt = verbatim_translation_measure(table, voc_dict)\n",
    "    m_pt = proximity_translation_measure(table, voc_dict)\n",
    "    m_km = keys_mapping_measure(table, voc_dict)\n",
    "    m_cm = chords_mapping_measure(table, voc_dict)\n",
    "    \n",
    "    if print_results:\n",
    "        print(\"%-24s: %-10.3f\" % (\"verbatim translation measure\", m_vt))\n",
    "        print(\"%-24s: %-10.3f\" % (\"proximity translation measure\", m_pt))\n",
    "        print(\"%-24s: %-10.3f\" % (\"keys mapping measure\", m_km))\n",
    "        print(\"%-24s: %-10.3f\" % (\"chords mapping measure\", m_cm))\n",
    "        \n",
    "    return((m_vt, m_pt, m_km, m_cm))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_results(n, pair, key_setup, score, measures):\n",
    "    \"\"\"\n",
    "    Write csv's to be further processed in R.\n",
    "    params:\n",
    "        n: context window with which co-occurrences were calculated\n",
    "        pair: list of pairs\n",
    "        key_setup: list of key setups\n",
    "        score: list of scores\n",
    "        measures: list of tuples of all measures\n",
    "    results:\n",
    "        r/data/measures.csv written\n",
    "    \"\"\"\n",
    "    \n",
    "    # Write measures\n",
    "    df = pd.DataFrame(\n",
    "        data = {\n",
    "            \"n\": n,\n",
    "            \"pair\": pair,\n",
    "            \"key_setup\": key_setup,\n",
    "            \"score\": score\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Column titles\n",
    "    titles_measures = [\n",
    "        \"m_vt\",\n",
    "        \"m_pt\",\n",
    "        \"m_km\",\n",
    "        \"m_cm\"\n",
    "    ]    \n",
    "    \n",
    "    # Iterate over results\n",
    "    for i in range(len(measures[0])):\n",
    "        measure_title = titles_measures[i]\n",
    "        measure_values = []\n",
    "            \n",
    "        for m in range(len(measures)):\n",
    "            measure_values.append(measures[m][i])\n",
    "            \n",
    "        # Add colum to dataframe\n",
    "        df[measure_title] = measure_values    \n",
    "    \n",
    "    # Save df to csv\n",
    "    title_df = \"measures_n=%d_%s\"% (n, datetime.now().strftime(\"%d_%m_%Y\"))\n",
    "    df.to_csv('r/data/'+title_df+'.csv', index=False)     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = read_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = preprocess(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Settings pipeline\n",
    "show_fig = True\n",
    "save_fig = True\n",
    "print_results = False\n",
    "\n",
    "# General info\n",
    "length_sequences = []\n",
    "\n",
    "# Data\n",
    "n = 5\n",
    "pairs = []\n",
    "setups = []    \n",
    "score = []\n",
    "measures = []\n",
    "\n",
    "# Pipeline for each pair, sorted by key setup\n",
    "for setup, data_setup in data.groupby('setup'):\n",
    "\n",
    "    for pair, data_pair in data_setup.groupby('pair'):\n",
    "    \n",
    "        # Reset index for each pair\n",
    "        data_pair = data_pair.reset_index(drop=True)\n",
    "    \n",
    "        # Title for graphs: pair number, key setup, score, date\n",
    "        title = \"n=%d-%s-%s-%s-%s\"%(n, pair, data_pair.loc[0, 'setup'], data_pair.loc[len(data_pair.index)-1, 'score'], datetime.now().strftime(\"%d_%m_%Y\"))\n",
    "        print(\"\\n%s %s\" % (title, \"-\"*78))\n",
    "    \n",
    "        # Measures\n",
    "        pairs.append(pair)\n",
    "        setups.append(setup)\n",
    "        score.append(data_pair.loc[len(data_pair.index)-1, 'score'])\n",
    "        \n",
    "        # A. PARSING\n",
    "        stream = parse(data_pair)\n",
    "        length_sequences.append([len(sequence) for sequence in stream])\n",
    "    \n",
    "        # B. CO-OCCURRENCES\n",
    "        m, voc_dict = matrix(stream, n)\n",
    "            \n",
    "        # Visualization embeddings\n",
    "        project_embeddings(title, m, voc_dict, save_fig, show_fig)\n",
    "    \n",
    "        # C. STATEMENTS\n",
    "        table = table_distances(title, m, voc_dict, save_fig, show_fig)\n",
    "        measures.append(calculate_measures(table, voc_dict, print_results))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write results to CSV's for further analysis in R\n",
    "write_results(n, pairs, setups, score, measures)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
